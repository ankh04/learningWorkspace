## 单词的向量化
任何一个颜色可以表示成色彩空间里的一个点, 即RGB表示法.

之所以能这样做是由相应的物理基础和生理基础:
- 光的颜色由频率(波长)决定, 波长短的偏紫, 波长长的偏红.
- 人的眼睛是感受色彩的生理基础, 人眼有三种视锥细胞, 分别识别三种颜色, 因此人们可以把任何一种颜色编程红, 绿, 蓝的混合

把一种概念向量化, 是使用计算机处理该概念的重要一步, 因为计算机本质上能做的事情就是数学运算.

如果希望使用计算机处理自然语言, 自然也需要把语言向量化. 而在自然语言处理领域中, 这项工作就称为*分布式表示*

## 分布式假设
> 某个单词的含义由它周围的单词形成

分布式假设认为,  单词本身没有含义, 单词含义由它的上下文(语境)形成.

上下文: 从该单词往左往右看多少个单词, 就是该单词上下文的*窗口大小*


## 共现矩阵
假设有一个由所有单词组成列表a:
{"abandon", "abandoned", ... "zygote", "zzz"}.

由这个列表a为索引的行和列构成的矩阵记为T

矩阵T的第i行第j列的元素代表: 第i行对应的单词与第j列对应的单词在**语料库**中一起出现的次数.
这样的矩阵T就是*共现矩阵*

共现矩阵的每一行, 就是改行对应单词的向量表示.


## 互点信息(PMI)
共现矩阵这样单纯**基于计数**的表示方式有个问题, 许多*语法结构词*, 比如定冠词, 语气词可能与许多词语都共同出现, 这样会干扰对词语的解析工作.

可以通过PMI来筛除那些与所有词关联性都很强的词.

### PMI的定义
如果单词 x 在语料库中出现的概率为p(x), 单词 y 在语料库中出现的概率为 p(y), 单词 x 和单词 y 共同出现的概率为 p(x, y),  那么 PMI定义为:
$$
PMI(x,y)=\log_2{P(x,y) \over P(x)P(y)} 
$$

## 向量的相似度
有了单词的向量化表示, 如何得知两个单词是含义相近的呢?

可以计算两个单词向量之间夹角的cos值, 如果这个值接近与1, 说明这两个单词很接近.

## 降维
上面无论是共现矩阵, 还是互点信息矩阵, 都是稀疏的, 意味着矩阵中有大量的位置都是0, 这样是很浪费计算机空间的.

为了更好的让计算机处理这些大矩阵, 需要对这些稀疏矩阵进行降维(压缩感知)

一种比较简单的方式是使用SVD

## PTB数据集
由托马斯-米克罗夫(Tomas Mikolov)提供的英语语料库

该库中讲具体的数字都替换成 N.
将稀有单词替换成特殊字符 \<unk\>
每个句子的结尾都是一个特殊字符\<eos\>


## 基于推理的方法: word2vec
上面的互点信息和共现矩阵都是*基于计数的方法*

而word2vec是*基于推理的方法*

word2vec 互点信息 共现矩阵都是基于*分布式假设的*


基于计数的方法, 这种方法的问题在于: 如果语料库中的单词特别多, 我们需要对一个很大的矩阵做SVD, 这个计算量是巨大的.

word2vec通过神经网络进行训练, 因此每次只需要对巨量的语料库中的 mini-batch 进行学习, 这意味着word2vec每次只看一部分学习数据, 从而使得对庞大语料库进行分析成为可能.

### 什么是推理?
所谓推理就是: 给出一个单词的上下文到神经网络, 让神经网络"预测"该单词, 这就是推理
![](https://picture-bed-1301848969.cos.ap-shanghai.myqcloud.com/20220524204258.png)

#### 神经网络的输出是什么?
和其他神经网络一样, 输出是一组"概率":
![](https://picture-bed-1301848969.cos.ap-shanghai.myqcloud.com/20220524204422.png)


基于推理的方法有两种网络结构: CBow 和 Skip-Gram

### CBow
CBow模型最后会得到两个矩阵 $W_{in}$和$W_{out}$, 最后的分布式表示可以有三种选项
- $W_{in}$的每一行可以作为单词的分布式表示
- $W_{out}$的每一列也可以作为单词的分布式表示
- 还可以把$W_{in}$和$W_{out}$两个矩阵综合起来作为单词的分布表示


目前最受欢迎的方案是" $W_{in}$的每一行可以作为单词的分布式表示"